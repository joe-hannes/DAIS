{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joe-hannes/DAIS/blob/main/week3/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c30802c2"
      },
      "source": [
        "# 4 Perceptron\n",
        "In this notebook we will want to look at the perceptron. What you have done manually in the previous task, we will now implement with Python and Numpy. Instead of choosing the weights by deliberation, first we will use the perceptron algorithm to learn the weights and later the backpropagation algorithm."
      ],
      "id": "c30802c2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "369eb8f8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "369eb8f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aed5b5"
      },
      "source": [
        "## 4.0 Recap\n",
        "We need to implement three major parts for our objective: a data set, the foward pass and the backward pass.\n",
        "\n",
        "Remember that our perceptron has to inputs, a bias and three learnable weights.\n",
        "\n",
        "![Perceptron](./fig/PerceptronG.jpg)"
      ],
      "id": "a5aed5b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c942e17"
      },
      "source": [
        "## 4.1 The Data\n",
        "Our aim is to learn the boolean function AND ($\\land$). For that purpose we can create our own data with the corresponding target/labels. We will use 0 for False and 1 for True.\n",
        "Also, the variable that holds the data will be denominated with _x_ and the variable holding the labels will be _t_.\n"
      ],
      "id": "4c942e17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58a43ba1"
      },
      "source": [
        "### Task 4.1.1 Create the data\n",
        "We will need a dataset that holds all possible inputs for the Boolean function AND. Here is a quick reminder on how that function works:\n",
        "\n",
        "| A| B | A AND B  |\n",
        "| :--- | :--- | --- |\n",
        "| True | False | True |\n",
        "| True | False | False |\n",
        "| False | True | False |\n",
        "| False | False | False |\n",
        "\n",
        "Our dataset needs to contain all four possible combinations of True and False. As we have a bias, we can treat it just like an input neuron with a fixed input of True.\n",
        "\n",
        "The labels should contain the teaching signal or ground truth for each dataset sample (as a vector), i.e. the correct output of the corresponding input.\n",
        "\n",
        "Both, input data and labels, should be represented as Numpy Matrices/Vectors."
      ],
      "id": "58a43ba1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "910f4b47"
      },
      "source": [
        "x = np.array([[1,1,1], [1,1,0], [1,0,1], [1,0,0]])"
      ],
      "id": "910f4b47",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f3d75a"
      },
      "source": [
        "t = np.array([1,0,0,0])"
      ],
      "id": "74f3d75a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1563dda"
      },
      "source": [
        "## 4.2 The Forward Pass\n",
        "Now we have to setup all necessary functions to complete a first forward pass through our perceptron and obtain a prediction based on the input.\n",
        "We need to be able to calculate the inner activation of the neuron _h_, define the activation function _g_, and finally calculate the outer activation _y_."
      ],
      "id": "d1563dda"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8321b367"
      },
      "source": [
        "### Task 4.2.1 Inner activation _h_\n",
        "From the lecture we now that the inner activation _h_ is just a weighted sum of the inputs.\n",
        "\n",
        "$$  \\large h_i = \\sum_{j=1}^{n} w_{ij} x_j $$\n",
        "\n",
        "Complete the function _inner_activation()_. As inputs it should take the input activations and the synaptic weights and return inner activation _h_."
      ],
      "id": "8321b367"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bef71e"
      },
      "source": [
        "def inner_activation():\n",
        "    # your code here"
      ],
      "id": "85bef71e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a926f37e"
      },
      "source": [
        "### Task 4.2.2 The activation function _g_\n",
        "The perceptron algorithm uses a step function as its activation function (sometimes also refered to as Heaviside function), to transfer the inner activation _h_ to the outer activation _y_. It looks as follows:\n",
        "\n",
        "$$    y_i = \n",
        "    \\begin{cases}\n",
        "      1 & \\text{if } h_i \\geq \\theta \\\\\n",
        "      0 & \\text{otherwise}\n",
        "    \\end{cases} $$\n",
        "    \n",
        "As the threshold, we define $\\theta$ to be 0.\n",
        "\n",
        "Complete the function _g()_. It should take the inner activation _h_ as an input and return the outer activation _y_."
      ],
      "id": "a926f37e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c646686c"
      },
      "source": [
        "def g():\n",
        "    # your code here"
      ],
      "id": "c646686c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37276c4f"
      },
      "source": [
        "### Task 4.2.3 Plot the activation function\n",
        "In order to check if we have done it correctly, we can plot the activation function.\n",
        "Create artificial datapoints in the range of -1 to 1, feed them into the activation function and plot the outputs."
      ],
      "id": "37276c4f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f8e5b77"
      },
      "source": [
        "# your code here\n",
        "plt.show()"
      ],
      "id": "9f8e5b77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f11798e"
      },
      "source": [
        "### Task 4.2.4 Putting it together\n",
        "Now we can combine our functions to compute the whole forward pass of the perceptron.\n",
        "\n",
        "Compete the function _forward_pass()_, which takes the input activations and the weights as inputs and returns the outer activation _y_."
      ],
      "id": "0f11798e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec9edad9"
      },
      "source": [
        "def forward_pass():\n",
        "    # your code here"
      ],
      "id": "ec9edad9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25da539f"
      },
      "source": [
        "## 4.3 The Backward Pass\n",
        "The backward pass changes the weights in respect to the error.\n",
        "\n",
        "From the lecture we now that the update rule of the perceptron algorithm looks like this:\n",
        "\n",
        "$$ \\large \\Delta w_{ij} = \\eta (t_i - y_i) x_j $$\n",
        "$$ \\large w_{ij} \\leftarrow w_{ij} + \\Delta w_{ij} $$\n",
        "\n",
        "$\\eta$ is a hyperparameter that needs to be set. In our case, 0.01 is a reasonable value."
      ],
      "id": "25da539f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58fc00fe"
      },
      "source": [
        "### Task 4.3.1 Update the weights\n",
        "Complete the function _update()_ which takes as input the current weights, the label, the input activation and the outer activation and returns the updated weights."
      ],
      "id": "58fc00fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cd7d458"
      },
      "source": [
        "def update():\n",
        "    # your code here"
      ],
      "id": "7cd7d458",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfa10293"
      },
      "source": [
        "## 4.4 Learning the boolean function\n",
        "Now we can use all the functions we have written, to piece together the perceptron algorithm and learn the AND function. Before we start, we need to set a start point in the weight space. For that purpose we define the weights more or less randomly before the learning starts."
      ],
      "id": "bfa10293"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4ada6d"
      },
      "source": [
        "w = np.array([3.0,-3.0,-2.0])"
      ],
      "id": "0a4ada6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e864871"
      },
      "source": [
        "### Task 4.4.1 The training loop\n",
        "In neural network training, you will often encouter a central training loop that iterates through the dataset and updates the weights regularly according to the generated predictions.\n",
        "Your task now is to write this training loop.\n",
        "\n",
        "For one epoch, step through all datapoints:\n",
        "  - compute the outer activation for the datapoint\n",
        "  - update the weights accordingly.\n",
        "  \n",
        "Do this for 1000 epochs.\n",
        "Every 50 epochs, print the current weights."
      ],
      "id": "2e864871"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dca48ef6"
      },
      "source": [
        "# your code here"
      ],
      "id": "dca48ef6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "993a8d1c"
      },
      "source": [
        "As you might have realised, the weights do not change anymore after a certain point. The perceptron algorithm has converged.\n",
        "We now have a set of weights, that we can use for predicting Boolean values. But before we deploy our model, we need to evaluate its quality. Has it really learned the function properly?"
      ],
      "id": "993a8d1c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8bff9f9"
      },
      "source": [
        "### Task 4.4.2 The error _E(w)_\n",
        "In order to evaluate how good our current set of weights _w_ is, we can compute the error.\n",
        "\n",
        "$$ \\large E(w) = \\frac{1}{2} \\sum_i (t_i - y_i)^2 $$\n",
        "\n",
        "Implement the function _error()_ which takes predcitions (outer activations) and (labels) as input and returns the error.\n",
        "\n",
        "Afterwards, compute the prediction error for the whole dataset. If it is zero, your model predicts perfectly."
      ],
      "id": "f8bff9f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bba1eb"
      },
      "source": [
        "def error(:\n",
        "    # your code here"
      ],
      "id": "78bba1eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d40970a"
      },
      "source": [
        "# getting the prediction error for the whole dataset\n",
        "preds = forward_pass(x,w)\n",
        "print(error(t,preds))"
      ],
      "id": "3d40970a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "129400c1"
      },
      "source": [
        "## 4.5 Deploy the model\n",
        "Now that we have a perfectly working and evaluated model, the most interesting part for the machine learning researcher is done. However, the end user, who might not be as familiar with neural networks as we are, might have trouble applying the model to new, unseen data."
      ],
      "id": "129400c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a62a61"
      },
      "source": [
        "### Task 4.5.1 The AND function\n",
        "We want to write a function that does not require the user to know about inner and outer activations or transfer functions but still enable them to harness the power of our model.\n",
        "\n",
        "Complete the function _AND()_ that takes as input too boolean values A and B and returns the prediction of our perceptron model."
      ],
      "id": "76a62a61"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2697c32f"
      },
      "source": [
        "def AND(a,b):\n",
        "    # your code here"
      ],
      "id": "2697c32f",
      "execution_count": null,
      "outputs": []
    }
  ]
}