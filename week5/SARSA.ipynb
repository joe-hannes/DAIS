{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9ad583",
   "metadata": {},
   "source": [
    "# 2 Tabular Reinforcement Learning\n",
    "In this notebook we are going to explore algorithms with which we can train agents to solve more or less simple environments on their own. Specifically, we will have a look at the SARSA algorithm. Instead of telling the agents what to do explicitely, we will only punish and reward their actions. Thereby, the agents should figure out a strategy by themselves, perhaps in ways that we would not have foreseen.\n",
    "\n",
    "Before we can start working on our agents, we have to set up an environment. The environment in a reinforcement learning problem is the world in which the agent acts. Games are very good example environments because they are sufficiently simple and can be simulated easily.\n",
    "\n",
    "## 2.1 The environment\n",
    "We will make use of the [gym](https://gym.openai.com/) library from openAI. It provides a number of environments with varying degrees of complexity in a convenient form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d885be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import gym\n",
    "from IPython import display\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe950e",
   "metadata": {},
   "source": [
    "In our task, we want to use the environment [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/). Let's have a look (because we are using Jupyter notebooks, we need to use the supplied function below to render the environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6252ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_render(env):\n",
    "    # render gym environments in jupyter notebooks\n",
    "    display.clear_output(wait=True)\n",
    "    env.render()\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596f790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset() # resets the environment\n",
    "nb_render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c575bb",
   "metadata": {},
   "source": [
    "In Taxi-V3, the agent is a taxi driver (yellow if empty, green if full) in a 5x5 square. The aim is to pick up a passenger (at the location in bold blue font) and drop the passenger off at the target location (in magenta).\n",
    "\n",
    "__Rewards__\n",
    "\n",
    "A successful drop-off gives a reward of 20. \n",
    "Attempting to pick-up or drop-off prematurely gives a negative reward (punishment) of -10.\n",
    "Each step taken also gives a negative reward of -1.\n",
    "\n",
    "__Actions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc464cc",
   "metadata": {},
   "source": [
    "The agent can execute one of six actions. Move north, south, east, west, pick-up or drop-off.\n",
    "\n",
    "__State-space__\n",
    "\n",
    "(States are called observations in gym lingo)\n",
    "\n",
    "We can encounter 500 different states in this environment (25 taxi positions * 5 passenger locations * 4 destination locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a3e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(env.observation_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40cb27",
   "metadata": {},
   "source": [
    "__Interaction__\n",
    "\n",
    "Interacting with the environment is really straight-forward. Simply supply the index of an action to the step-function of the environment.\n",
    "As return values, you will receive the next state, the reward for the action and if the environment is solved (you can ignore the fourth return value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe24fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s, r, done, _ = env.step(3)\n",
    "nb_render(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc7ad4f",
   "metadata": {},
   "source": [
    "## 2.2 Evaluation of an agent\n",
    "Since we want to have a _successful_ agent, we need a measure to evaluate that success. The cumulative reward over the whole episode should suffice for that purpose. An episode is the whole interaction process between agent and environment until the problem is solved or otherwise interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f67c7",
   "metadata": {},
   "source": [
    "### Task 2.2.1 Random agent\n",
    "In order to test our evaluation and compare our agent later, we need to have a baseline agent.\n",
    "Since we do not know any better at the moment, let's implement an agent that acts according to a random policy.\n",
    "Write a function _random\\_policy()_ that takes the current state as input and returns a random action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ca66a",
   "metadata": {},
   "source": [
    "### Task 2.2.1 Evaluation function\n",
    "Write a function _evaluate()_ that takes a policy function as input, runs the policy on 20 independent episodes of the environment and returns a list of the cumulative rewards of those episodes.\n",
    "\n",
    "_Hint_: The _reset()_ function should be run before each episode starts and provides you with the first state.\n",
    "\n",
    "In order to see what our agent is doing, render the last episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c354be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(policy):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1691c",
   "metadata": {},
   "source": [
    "### Task 2.2.3 Evalute the random agent\n",
    "Run the evaluation scheme on the random agent and plot the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d951c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54582262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99344661",
   "metadata": {},
   "source": [
    "## 2.3 SARSA\n",
    "Now we want to write an agent that learns from rewards and in the end performs the task a little better.\n",
    "From the lecture, we can remember that the SARSA algorithm deals with Q values. These values tell us how good it is to take a specific action in a specific state. For descrete problems up to a certain complexity (as is the Taxi-v3) we can simply store the Q values in a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d2d3c",
   "metadata": {},
   "source": [
    "### Task 2.3.1 The Q-Table\n",
    "Write the function _init\\_table()_ that sets up and returns a table that contains the Q values of state-action pairs.\n",
    "Initialise all Q values with zeros.\n",
    "Over time, while learning, the agent will change these values in order to prefer certain actions in certain situations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_table():\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb61ec",
   "metadata": {},
   "source": [
    "### Task 2.3.2 Exploitation: greedy strategy\n",
    "The Q table represents our knowledge about the goodness of actions. Usually, we want to choose the best action for a certain situation. This way of choosing actions can be seen as exploiting your knowledge.\n",
    "Write a function _exploit()_ that returns the best action for a given state according to your Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit(state):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b91c11",
   "metadata": {},
   "source": [
    "### Task 2.3.3 Exploitation vs. Exploration\n",
    "However, greedily executing actions all the time is not the best idea. A big problem in reinforcement learning is sufficent exploration of the environment. You will never know if your current strategy is optimal, if you did not fully explore all possibilities. But exploring is costly, as it means to deviate from exploiting your current strategy.\n",
    "\n",
    "One way to tackle this dilemma is the $\\epsilon$-greedy strategy. For a certain fraction of actions $\\epsilon$, we will not use our knowledge to choose the best action, but rather choose a random non-greedy action to explore the environment.\n",
    "\n",
    "Write a function _explore()_ that returns a random non-greedy action for a given state according to your Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacaf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(state):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378d349",
   "metadata": {},
   "source": [
    "Write a function _eps\\_greedy()_ that takes a state as input. In $\\epsilon$ of all cases return an explorative action and in 1-$\\epsilon$ of all cases return an exploitative action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc940ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "def eps_greedy(state):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918193e",
   "metadata": {},
   "source": [
    "### Task 2.3.1 The temporal difference error\n",
    "We only need one last piece to complete our algorithm: updating our knowledge about good state-action combinations. Hence, we have to update our Q-values in a way that reflects how good an action is to reach the goal in the future.\n",
    "\n",
    "The TD-error for SARSA is given as (see lecture p.19):\n",
    "\n",
    "$$ \\delta =  r + \\gamma Q(s';a') - Q(s;a) $$\n",
    "\n",
    "Write a function _td\\_error()_, that takes as arguments the current state, the taken action, the reward, the next state and the next action and returns the temporal difference error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def td_error(s, a, r, s_prime, a_prime):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879e9181",
   "metadata": {},
   "source": [
    "Lastly, the Q table needs to be updated according to:\n",
    "    $$ Q(s;a) \\leftarrow Q(s;a) + \\eta \\delta $$\n",
    "    \n",
    "Write a function _update\\_table()_ that takes as inputs the current state, the current action and the td-error and updates the Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "\n",
    "def update_table(s, a, delta):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08c0b4",
   "metadata": {},
   "source": [
    "## 2.4 Training\n",
    "Now we can finally train our agent. 2000 episodes should suffice to sufficiently solve the Taxi problem.\n",
    "\n",
    "The SARSA algorithm for one episode goes as follows:\n",
    "\n",
    "- initialise environment, read initial state s\n",
    "- select an action a\n",
    "- repeat until done:\n",
    "    - execute action $a$\n",
    "    - read reward $r$ and new state $s'$\n",
    "    - select next action $a'$\n",
    "    - compute TD-error\n",
    "    - update Q table\n",
    "    - set variables for next iteration: \n",
    "        $s \\leftarrow s'$, $a \\leftarrow a'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab04df",
   "metadata": {},
   "source": [
    "### Task 2.4.1 The train function\n",
    "Write a train function that applies the SARSA algorithm to the environment for 2000 episodes. The function should return a list of cumulative rewards for each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = init_table()\n",
    "episodes = 2000\n",
    "\n",
    "def train():\n",
    "    # your code here\n",
    "\n",
    "train_rewards = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba01560",
   "metadata": {},
   "source": [
    "### Task 2.4.2 Visualise the training\n",
    "Train your agent and obtain the cumulative rewards for the episodes.\n",
    "Plot the temporal evolution of the rewards over the episodes in order to see if the training worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b03f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570409f0",
   "metadata": {},
   "source": [
    "## 2.5 Results\n",
    "Now that we have a trained agent it is time to evaluate the trained agent and compare it against the performance of the random agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8647a87",
   "metadata": {},
   "source": [
    "### Task 2.5.1 Evaluate the trained agent\n",
    "We already wrote the evaluation function. So now we just need to apply our policy to the environment and see how the agent performs.\n",
    "It is perhaps best to just use the greedy policy. If we assume that our agent is fully trained to the best of our knowledge, and we are not updating our Q table and thus are not learning during evaluation, there is no need for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd2431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe5218",
   "metadata": {},
   "source": [
    "### Task 2.5.2 Compare the agents\n",
    "Plot the results of the random agent vs the results of the trained agent for a nice comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
