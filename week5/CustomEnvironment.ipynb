{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f9b6dc",
   "metadata": {},
   "source": [
    "# 3 Custom Environments\n",
    "In this task you are asked to create a environment for a reinforcement agent. It's common to create environments for agents by using the openai gym interface. It creates a good baseline for what is necessary to train a RL agent and makes it easy to try out different environments on the same algorithm.\n",
    "If you need more information take a look at the documentation https://gym.openai.com/docs/.\n",
    "You can find the implementation of all official enviroments on GitHub: https://github.com/openai/gym/tree/master/gym/envs if you need some examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12a89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7d886",
   "metadata": {},
   "source": [
    "## 3.1 The Environment\n",
    "A openai gym environment consists of at least 3 methods. `__init__` the constructor which sets all the necassary values, a `step` function, which describes the behaviour of the environment and a `reset` function, which resets the starting state of the environment. In addition to that usually a `render` function is provided to visualize the behaviour of the environment.\n",
    "\n",
    "#### \\__init__:\n",
    "The Constructor of the environment defines all the necassary variables. To set the bounds of our environment we have to define the action_space and the observartion space. The gym.spaces library contains the necassary functions to do in our case we use gym.spaces.Discrete because we wan to only handle discrete values.\n",
    "The Discrete space works a bit like a range, with some extra methods. A Linear representation of the state is helpful for tabular learning, because it makes creating a Q-Table really easy. \n",
    "\n",
    "#### step:\n",
    "The step method takes an action and returns a tuple of the shape(observation, reward, done). The observation is the result of taking the action. The reward is the reward handet for takin given action in the previous state. The done variable is boolean and indicates if a given scenario has come to an end. \n",
    "\n",
    "#### reset:\n",
    "The reset method rests the start state of the environment. It returns the new state of the environment.\n",
    "\n",
    "#### render:\n",
    "The render method visualizes the state of the environment. There are many different ways to do so i.e. creating a visual representation by using vector graphics or printing to the terminal. \n",
    "We want to focus on the easiest way, by printing the state. Find a good and easily printable representation of the internal state (i.e. a numpy array) and print it. To print over the last output you can call the  function before you print the state.\n",
    "\n",
    "### Encoding and decoding\n",
    "This functions are not necessary for a gym environment. However it might be usefull do write some functions that encode and decode the linearized state to a 2D imensional Form and back.\n",
    "\n",
    "#### decode_action\n",
    "returns the action refering to the index of the action\n",
    "\n",
    "#### decode_state\n",
    "returns a 2D representation of the linear state\n",
    "\n",
    "#### encode_state\n",
    "returns a linear representation of the 2D state.\n",
    "\n",
    "\n",
    "### Task 3.1.1\n",
    "- Create a two dimensional, discrete environment of the size 8x8.\n",
    "- Each episode the agent should start at a random position, while the target always stays at the same position.\n",
    "- The agent should be able to move in all 4 directions, If the agent hits a wall it should do nothing.\n",
    "- An episode ends if the agent reaches the target.\n",
    "- Reaching the target results in a reward of 1, while every other action should give a small negative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        # Write a constructor for your enviroment\n",
    "        # Define the action_space and observation_space\n",
    "        # Position your agent and the target in the enviroment \n",
    "        \n",
    "    def step(self, action):\n",
    "        # Write the step method for your enviroment. Make sure you agent does not go out of bounds\n",
    "        # by performing the action.\n",
    "        \n",
    "        return (observation, reward, done,_)\n",
    "     \n",
    "    def reset(self):\n",
    "        # Write the reset method that results in the starting state\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def render(self):\n",
    "        #Write a render method for your enviroment to visualize the current state in the terminal\n",
    "\n",
    "    def decode_action(self, action):\n",
    "        #decode a linear action to 2D\n",
    "        return \n",
    "\n",
    "    def decode_state(self, state):\n",
    "        #decode a linear state to 2D\n",
    "        return \n",
    "    \n",
    "    def encode_state(self,state):\n",
    "        #encode a 2D state in 1D\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcdafe",
   "metadata": {},
   "source": [
    "## 3.2 Test with a random agent\n",
    "The following cell allows you to test your enviroment with a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomEnv()\n",
    "done = False\n",
    "while done == False:\n",
    "    a = env.action_space.sample()\n",
    "    _,_, done,_ = env.step(a)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73e00f",
   "metadata": {},
   "source": [
    "## Task 3.3 Test with a Q-Learning Agent\n",
    "In the previous task we wrote an agent that used the SARSA algorithm. Now we want to use a similar algorithm, Q-Learning, to solve your own custom environment. And of course visualise your training progress (Cumulative rewards over time).\n",
    "\n",
    "The main difference between SARSA and Q-Learning is the way the Q-Values are calculated. Therefore, you can recycle most of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea757dd",
   "metadata": {},
   "source": [
    "### Task 3.3.1 Size concerns for Tabular RL:\n",
    "The table for learning our simple enviroment has the size 64x4 for now. Since we have 64 possible States and 4 actions. How much bigger would the table get if we allowed the target to be placed anywhere?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a53e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
